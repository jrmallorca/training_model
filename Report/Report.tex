\documentclass[a4paper,11pt]{article}
\usepackage[total={6in, 9in}]{geometry}
\usepackage[font=small,labelfont=bf,skip=2pt]{caption} % example skip set to 2pt
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{tabularx}
\usepackage{siunitx}
\usepackage{amsmath}

\sisetup{
	round-mode = places,
	round-precision = 2,
}

\begin{document}

\title{Reconstructing Signals via Least Squares Error Modelling}
\author{Jonquil Isys R. Mallorca}
\date{\today}
\maketitle

\section{Introduction}
The Least Squares Method (LSM) is a standard procedure in regression analysis to approximate a solution for a set of coordinates. It involves minimising the residual sum of squares (RSS) of every coordinate to find the best fitting line.
\\ \\
Cross-validation (CV) is a resampling procedure that splits a set of points, trains the model using the training points and checks the cross-validation error (CVE) with the test points. The variation used here is $k$-fold cross-validation which repeats the process stated $k$ times and retrieves the function type with the lowest mean cross-validation error.
\\ \\
LSM and CV will be used, with the data given, to train our model and observe the results. We expect the program to be able to pick a suitable model, based on the points plotted, with minimal RSS.

\section{Explanation of the program}
The program reads in multiple segments of points from a file\footnote{Note that utilities.py is not used as the functions related to it have been copied over to the main file and modified.}. Through LSM and CV, it will find the function type that appropriately fits the segment, alongside its sum of squares error (SSE). The SSE of each segment is then summed up to form the RSS.
\\ \\
The model has three equations of a line to choose from: linear (degree 1), cubic (degree 3) and sinusoidal (we discuss these selections in Subsection \ref{compare}).

	\subsection{Linear \& Polynomial Regression}
	\label{lin_poly}
	Polynomial, and thus linear, regressions can be acquired through the general LSM in matrix form. The vector column of y-coordinates $y$ and the matrix $X$, where each row represents the variables of the terms in the equation of a polynomial, are inputted.
	%
	\begin{equation*}
	y_{(N\times1)} = 
		\begin{bmatrix}
		y_1 \\ 
		\vdots \\
		y_N
		\end{bmatrix} \;
	X_{(N\times(p + 1))} = 
		\begin{bmatrix}
		1 & x_1^1 & \cdots & x_1^p \\ 
		\vdots & \vdots & \ddots & \vdots \\
		1 & x_N^1 & \cdots & x_N^p
		\end{bmatrix}
	\end{equation*}
	%
	where
	%
	\begin{equation*}
	p = 
		\begin{cases}
	      1, & \text{if polynomial is linear} \\
	      3, & \text{if polynomial is cubic}
	    	\end{cases}
	\end{equation*}
	%
	Using the input, we compute the coefficient vector $A = (X^tX)^{-1}X^ty$, and regression line $\hat{y}$, where $\hat{y}_i = a_0 + a_1x_i + a_2x_i^2 + \cdots + a_px_i^p$ and each $a_i$ are components of $A$.
	
	\subsection{Sinusoidal Regression}
	Using similar steps in subsection \ref{lin_poly}, We input the y-coordinates $y$ from a file and a matrix $X$ where each row represents the variables of the terms in the equation of a polynomial.
	%
	\begin{equation*}
	y_{(N\times1)} = 
		\begin{bmatrix}
		y_1 \\ 
		\vdots \\
		y_N
		\end{bmatrix} \;
	X_{(N\times2)} = 
		\begin{bmatrix}
		1 & sin(x_1) \\
		\vdots & \vdots \\
		1 & sin(x_N)
		\end{bmatrix}
	\end{equation*}
	%
	With the input, we calculate the coefficient vector $A = (X^tX)^{-1}X^ty$, and regression line $\hat{y}$, where $\hat{y}_i = a_0 + a_1sin(x_i)$ and each $a_i$ are components of $A$.

\section{Results}
	\subsection{Results from chosen regression functions}
	As demonstrated in Table \ref{tab:rss}, the model chooses agreeable functions for each file segment, even when noise is present. Thus, the model correctly prioritises on selecting the most appropriate function type with relatively low RSS.

	\begin{table}[ht]
	\scriptsize\centering
	\caption{\label{tab:rss} RSS and sequence of function types for the line segments of each file}
		\begin{tabular}[t]{
			p{\dimexpr.25\linewidth-2\tabcolsep-1.3333\arrayrulewidth}% column 1
			p{\dimexpr.25\linewidth-2\tabcolsep-1.3333\arrayrulewidth}% column 2
			p{\dimexpr.5\linewidth-2\tabcolsep-1.3333\arrayrulewidth}% column 3
		}
		\toprule
		\textbf{File} & \textbf{RSS} & \textbf{Regression functions used}\\ \midrule
		basic\textunderscore1 & \num{1.6881623371729653e-28} & Linear \\
		basic\textunderscore2 & \num{6.214646211331131e-27} & Linear, Linear \\
		basic\textunderscore3 & \num{1.4384831613188792e-18} & Cubic \\
		basic\textunderscore4 & \num{1.3851152714485016e-12} & Linear, Cubic \\
		basic\textunderscore5 & \num{1.050131637030211e-25} & Sine \\
		adv\textunderscore1 & \num{199.72560131252334} & Cubic, Linear, Cubic \\
		adv\textunderscore2 & \num{3.685132050447601} & Sine, Linear, Sine \\
		adv\textunderscore3 & \num{1019.4370683681474} & Linear, Cubic, Sine, Linear, Sine, Cubic \\
		noise\textunderscore1 & \num{12.207460140137117} & Linear \\
		noise\textunderscore2 & \num{849.5527462320404} & Linear, Cubic \\
		noise\textunderscore3 & \num{482.9090507852767} & Linear, Cubic, Sine \\ \bottomrule
		\end{tabular}
	\end{table}
	
	\noindent
	However, cross-validation cannot completely eradicate overfitting. This is evidently shown on Table \ref{tab:diff_rss} and Table \ref{tab:diff_func}. It was discovered that the program was frequently alternating between linear and sine functions to model the first segment as the mean CVE would differ depending on how the test set was chosen. Figure \ref{fig:adv_3_normal} shows that the first segment is obviously linear.

	\begin{table}[ht]
	\scriptsize\centering
	\caption{\label{tab:diff_rss} Different RSS results for adv\_3}
		\begin{tabularx}{\textwidth}[t]{X X X X}
		\toprule
		\textbf{File} & & RSS & \\
		& (Optimal) & (Worse) & (Worst) \\ \midrule
		adv\textunderscore3 & \num{1019.4370683681474} & \num{1017.6979899741879} & \num{1005.1147393985742} \\ \bottomrule
		\end{tabularx}
	\end{table}
	%
	\begin{table}[ht]
	\scriptsize\centering
	\caption{\label{tab:diff_func} Different regression functions used for adv\_3}
		\begin{tabularx}{\textwidth}[t]{X X X X}
		\toprule
		\textbf{File} & & RSS & \\
		& (Optimal) & (Worse) & (Worst) \\ \midrule
		adv\textunderscore3 & Linear, Cubic, Sine, Linear, Sine, Cubic & Sine, Cubic, Sine, Linear, Sine, Cubic & Sine, Cubic, Sine, Cubic, Sine, Cubic \\ \bottomrule
		\end{tabularx}
	\end{table}

	\subsection{\label{compare} Comparison to other possible function types}
	The reasons why a cubic polynomial was preferred compared to other potential polynomials are displayed in Table \ref{tab:compare}. When $p = 2$, most files had either the same or higher RSS than when $p = 3$, indicating underfitting. In comparison, when $p = 4$, certain files had lower RSS than when $p = 2$; however, other files gained a large increase in RSS, most notably adv\_3.csv.
	%
	\begin{table}[ht]
	\scriptsize\centering
	\caption{\label{tab:compare} Comparison of RSS when degree of polynomial $p = 2, 3, 4$}
		\begin{tabularx}{\textwidth}[t]{X X X X}
		\toprule
		\textbf{File} & \textbf{RSS ($p = 2$)} & \textbf{RSS ($p = 3$)} & \textbf{RSS ($p = 4$)}\\ \midrule
		basic\textunderscore3 & \num{15.743318683748834} & \num{1.4384831613188792e-18} & \num{2.6012776884995257e-12} \\
		adv\textunderscore3 & \num{1014.409779528759} & \num{1019.4370683681474} & \num{91521.01820439952} \\ \bottomrule
		\end{tabularx}
	\end{table}
	
	\noindent
	In the last data segment of Figure \ref{fig:adv_3}, the program chose to use a linear function type to fit the data as it had the lowest CVE when $p = 4$. It is clear that using $p = 4$ or higher would lead to overfitting and errors.
	%
	\begin{figure}[ht]
		\begin{minipage}[c]{0.5\linewidth}
		\centering
		\includegraphics[width=1\linewidth, valign=t]{../figs/adv3normal.png}
		\caption{Graph of adv\_3.csv when $p = 3$}
		\label{fig:adv_3_normal}
		\end{minipage}
		%
		\begin{minipage}[c]{0.5\linewidth}
		\centering
		\includegraphics[width=1\linewidth, valign=t]{../figs/adv3.png}
		\caption{Graph of adv\_3.csv when $p = 4$}
		\label{fig:adv_3}
		\end{minipage}
	\end{figure}
	%
\section{Conclusion}
Choosing linear, cubic and sinusoidal regressions, with CV, as the three primary ways of discerning the nature of every segment is sufficient in generalising the data while minimizing overfitting/underfitting, with respect to the training files given. 
\\ \\
As a future improvement, regularisation could be used with cross-validation to further the accuracy of the program.
\end{document}